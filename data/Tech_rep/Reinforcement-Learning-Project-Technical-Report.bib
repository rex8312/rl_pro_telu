Automatically generated by Mendeley Desktop 1.19.3
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Plappert2017,
abstract = {Deep reinforcement learning (RL) methods generally engage in exploratory behavior through noise injection in the action space. An alternative is to add noise directly to the agent's parameters, which can lead to more consistent exploration and a richer set of behaviors. Methods such as evolutionary strategies use parameter perturbations, but discard all temporal structure in the process and require significantly more samples. Combining parameter noise with traditional RL methods allows to combine the best of both worlds. We demonstrate that both off- and on-policy methods benefit from this approach through experimental comparison of DQN, DDPG, and TRPO on high-dimensional discrete action environments as well as continuous control tasks. Our results show that RL with parameter noise learns more efficiently than traditional RL with action space noise and evolutionary strategies individually.},
archivePrefix = {arXiv},
arxivId = {1706.01905},
author = {Plappert, Matthias and Houthooft, Rein and Dhariwal, Prafulla and Sidor, Szymon and Chen, Richard Y. and Chen, Xi and Asfour, Tamim and Abbeel, Pieter and Andrychowicz, Marcin},
doi = {10.1063/1.334137},
eprint = {1706.01905},
file = {:home/theo/study/reinforcement{\_}learning/Tech{\_}rep/Literature/2018{\_}Plappert{\_}ParameterNoiseForExploration.pdf:pdf},
isbn = {9781441914279},
issn = {00218979},
pages = {1--18},
pmid = {17758941},
title = {{Parameter Space Noise for Exploration}},
url = {http://arxiv.org/abs/1706.01905},
year = {2017}
}
@article{Abdolmaleki2018a,
abstract = {We introduce a new algorithm for reinforcement learning called Maximum aposteriori Policy Optimisation (MPO) based on coordinate ascent on a relative entropy objective. We show that several existing methods can directly be related to our derivation. We develop two off-policy algorithms and demonstrate that they are competitive with the state-of-the-art in deep reinforcement learning. In particular, for continuous control, our method outperforms existing methods with respect to sample efficiency, premature convergence and robustness to hyperparameter settings while achieving similar or better final performance.},
archivePrefix = {arXiv},
arxivId = {1806.06920},
author = {Abdolmaleki, Abbas and Springenberg, Jost Tobias and Tassa, Yuval and Munos, Remi and Heess, Nicolas and Riedmiller, Martin},
eprint = {1806.06920},
file = {:home/theo/study/reinforcement{\_}learning/Papers/Policy{\_}Search/2018{\_}Abdolmaleki{\_}MaximumAPosterioriPolicyOptimisation.pdf:pdf},
month = {jun},
title = {{Maximum a Posteriori Policy Optimisation}},
url = {http://arxiv.org/abs/1806.06920},
year = {2018}
}
@article{Lillicrap2015,
abstract = {We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies end-to-end: directly from raw pixel inputs.},
archivePrefix = {arXiv},
arxivId = {1509.02971},
author = {Lillicrap, Timothy P. and Hunt, Jonathan J. and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
doi = {10.1561/2200000006},
eprint = {1509.02971},
file = {:home/theo/study/reinforcement{\_}learning/Papers/2015{\_}Lillycrap{\_}ContinousControlWithDeepReinforcementLearning.pdf:pdf},
isbn = {2200000006},
issn = {1935-8237},
pmid = {24966830},
title = {{Continuous control with deep reinforcement learning}},
url = {http://arxiv.org/abs/1509.02971},
year = {2015}
}
@article{Silver2014,
author = {Silver, David and Lever, Guy and Technologies, Deepmind and Lever, G U Y and Ac, U C L},
file = {:home/theo/study/reinforcement{\_}learning/Tech{\_}rep/Literature/silver14.pdf:pdf},
isbn = {9781634393973},
issn = {1938-7228},
pages = {1--9},
title = {{Silver14}},
url = {papers://d471b97a-e92c-44c2-8562-4efc271c8c1b/Paper/p652},
year = {2014}
}
@article{Peters2008a,
author = {Peters, Jan and Katharina, M and Alt, Yasemin},
file = {:home/theo/study/reinforcement{\_}learning/Tech{\_}rep/Literature/Peters2010{\_}REPS.pdf:pdf},
title = {{Relative Entropy Policy Search the KL in the other}},
year = {2008}
}
